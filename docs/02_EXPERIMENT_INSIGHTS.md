# 실험 인사이트 및 교훈

> "때로는 첫 시도가 최선이라는 겸손함을 배운 여정"

## 개요

6단계(Phase 1-6)에 걸친 체계적 실험을 통해, 프롬프트 엔지니어링의 양날의 검을 경험했다.
Train 데이터에서 좋은 결과가 나오면 Test에서 더 나빠지고, 더 많은 예시를 추가하면 성능이 급락하고,
명확한 규칙을 추가해도 이미 처리되어 있고...

결국 첫 번째 시도인 Baseline 34.04%를 넘지 못했다. 하지만 이 실패의 과정에서 얻은 교훈이
숫자보다 더 가치 있었다.

---

## 전체 실험 여정

### Phase 1: 예상치 못한 출발 (2025-10-22)

**첫 제출에서 34.04% 달성**

단순한 구조의 Baseline 프롬프트:
- System message로 역할 부여
- 맞춤법 예시 **1개만** Few-shot
- 특별한 기교 없음

당시에는 이것이 최고점이 될 줄 몰랐다. "개선할 여지가 많아 보인다"고 생각했다.

**Technical Details:**
```
Public LB: 34.0426%
Private LB: 13.4454%
Train Recall: 32.24%

사용한 예시:
원문: 오늘 날씨가 않좋은데, 김치찌게 먹으러 갈려고.
교정: 오늘 날씨가 안 좋은데, 김치찌개 먹으러 가려고.
```

### Phase 2: "더 많으면 더 좋지 않을까?" (2025-10-23)

#### 실험 2-1: Zero-shot (예시 0개)
**가설**: 예시 없이 더 일반적으로 작동하지 않을까?

**결과**: 31.91% Public (-2.13%p)

**발견**: 너무 보수적. 확실하지 않으면 원문 유지하는 경향. False Negative 증가.

#### 실험 2-2: Plus3 (예시 4개)
**가설**: 다양한 예시를 4개 보여주면 더 잘 배우지 않을까?

**결과**: 27.66% Public (-6.38%p, **18.7% 하락**)

**충격적 발견**:
- 긴 문장의 **첫 문장만 출력**하고 멈춤 (3개 케이스에서 70-80% 텍스트 삭제)
- Few-shot 예시가 모두 짧은 단일 문장이었기 때문
- 모델이 "이 정도 길이면 됐구나" 학습

**예시 (grm807505):**
```
원문 (245자, 3문장):
바이든 행정부 출범 이래 대만의 독자 핵무장에 대한 국민적 관심과 지지가 증가 추세다.
2017년 중국의 제6차 핵실험 이후 주로 보수적 시민단체들에 의해 제기되기 시작한 핵무장 담론은...
(... 계속)

Plus3 교정 (48자, 1문장만):
바이든 행정부 출범 이래 대만의 독자 핵무장에 대한 국민적 관심과 지지가 증가 추세다.
```

**치명적 부작용**:
- 띄어쓰기 교정 소홀 (Few-shot에 띄어쓰기 예시 없음)
- 문장부호 교정 소홀
- 심지어 사실 오류 생성 ("함경도" → "함경**남**도", 실제로는 함경북도가 맞음)

**교훈**: 더 많은 예시 ≠ 더 좋은 성능. 오히려 특정 패턴에 과적합.

### Phase 3: "특화하면 되지 않을까?" (2025-10-23)

#### 실험 3-1: 조사 특화 예시
**가설**: Train 데이터에서 조사 오류가 많으니, 조사 예시로 특화하면?

**Train 성능**: 33.47% (+1.23%p 향상!)
**기대**: "Train에서 좋으니 Test도 좋겠지!"

**결과**:
- Public: 31.91% (-2.13%p)
- Private: 11.54% (**최저 성적**)

**발견**: Train 성능과 Test 성능이 **완전히 반대 방향**

이 실험에서 깨달았다: **Train 성능은 Test의 지표가 아니다.**

#### 실험 3-2: 띄어쓰기 특화 예시
**Train 검증**: 32.65%, **길이 폭발 49개 (19.3%)**

즉시 폐기. LB 제출조차 안 함.

**교훈**: 예시 다양성 > 특화. 맞춤법 예시(3가지 오류 유형 포함)가 조사 예시(1가지만)보다 우수.

### Phase 4-5: 깨달음의 시간 (2025-10-23 ~ 24)

**패턴 분석 결과**:
```
예시 개수 vs LB 성능:
0개 (Zero-shot): 31.91 / 12.61 - 보수적
1개 (Baseline):  34.04 / 13.45 - 최적점 [중요]
4개 (Plus3):     27.66 / 9.77  - 과적합

Train vs Test 상관관계:
Plus3: Train 34.69% → Public 27.66% (-7.03%p)
조사:  Train 33.47% → Public 31.91% (-1.56%p)
```

**핵심 발견**:
1. Few-shot은 1개가 최적점
2. Train 성능 향상 = Test 성능 하락
3. Public/Private 격차 20%p (구조적 문제, 데이터 분포 차이)

### Phase 6: 마지막 시도 (2025-10-24)

#### 규칙 기반 후처리 실험

**전략**: Train 데이터에서 명확한 패턴 추출 → 규칙으로 보정

**추출한 규칙 3개**:
1. `금새` → `금세` (100% 확실)
2. `치 않` → `지 않` (형용사 뒤, 95%+ 확실)
3. `추측컨대` → `추측건대` (100% 확실)

**안전장치**:
- Baseline이 이미 교정한 것은 건드리지 않음
- 60% 길이 가드 (과도한 삭제 방지)
- 150% 길이 가드 (과도한 추가 방지)

**결과**:
```
규칙 적용: 0개
Recall: 32.24% (변화 없음)
```

**왜?**
Train 데이터에 패턴이 분명 존재함 (grep으로 확인). 하지만 Baseline이 **이미 모두 교정**했기 때문에
적용할 대상이 없었음.

**최종 깨달음**:
Baseline의 한계는 "명확한 규칙을 못 잡아서"가 아니라,
"복잡한 맥락 이해"와 "애매한 오류 판단"이 필요한 부분이었다.

---

## 주요 실패 사례 심층 분석

### 실패 1: Zero-shot의 보수주의

**왜 실패했나?**

예시가 없으니 모델이 불확실성을 회피:
- "이게 오류인지 확신이 안 서" → 원문 유지
- False Negative 증가
- Recall 하락

**수치 증거**:
- Baseline 평균 길이: 171.5자
- Zero-shot 평균 길이: 162.6자 (-9자, 덜 교정함)

### 실패 2: Plus3의 파국적 과적합

**3가지 치명적 문제**:

#### 문제 1: 첫 문장만 출력

Few-shot 예시가 모두 짧은 단일 문장 → 모델이 학습
```python
예시 1: "오늘 날씨가 않좋은데..." (1문장, 25자)
예시 2: "그는 삼성전자의 제안을..." (1문장, 22자)
예시 3: "그녀는 개나리꽃길을..." (1문장, 21자)
예시 4: "이 제품은 정말 최고의..." (1문장, 29자)
```

결과: 긴 문장 3개 케이스에서 70-80% 텍스트 삭제

#### 문제 2: 띄어쓰기 교정 소홀

Few-shot 예시에 띄어쓰기 오류 거의 없음 → 띄어쓰기 중요도 낮게 학습

비교:
```
원문: "받아봐야겠다"
Baseline: "받아 봐야겠다" [완료]
Plus3: "받아봐야겠다" [실패] (그대로)
```

#### 문제 3: 사실 오류 생성

```
원문: "함경도 청진"
Plus3: "함경남도 청진" [실패]
(청진은 함경북도가 맞음)
```

모델이 과신하여 확실하지 않은 정보 추가.

**Recall 관점 분석**:
```
Recall = TP / (TP + FP + FN) × 100

Plus3 문제:
- False Negative (FN) 대폭 증가 (띄어쓰기 등 놓침)
- True Positive (TP) 감소 (첫 문장만 출력)
→ 분자 감소 + 분모 증가 = Recall 급락
```

### 실패 3: 조사 특화의 역설

**Train에서 성공 → Test에서 참패**

```
Train Recall: 33.47% (Baseline 대비 +1.23%p)
Public LB: 31.91% (Baseline 대비 -2.13%p)
Private LB: 11.54% (Baseline 대비 -1.91%p, 최저)
```

**원인**:
1. 조사 오류에만 집중 → 다른 오류 유형 소홀
2. Train 데이터의 조사 오류 분포 ≠ Test 데이터
3. 일반화 실패

**교훈**: 특화 > 다양성? 아니다. **다양성이 곧 일반화 능력**.

### 실패 4: 규칙 기반의 무력함

**패러독스**:
- Train 데이터 분석 → 명확한 패턴 발견
- 100% 확실한 규칙 3개 추출
- 적용 결과: 0개

**이유**: Baseline이 이미 완벽.

**시사점**:
- Baseline 34%는 규칙을 못 잡아서가 아님
- 진짜 한계는 규칙으로 표현할 수 없는 부분
- 맥락, 뉘앙스, 주관적 판단이 필요한 오류들

---

## 성공한 유일한 접근법: Baseline

### Baseline의 비밀

**겉보기**: 평범한 1-shot 프롬프트
**실제**: 우연히 완벽한 균형점

#### 성공 요인 1: 예시의 다양성

단 1개 예시에 **3가지 오류 유형** 포함:
```
원문: 오늘 날씨가 않좋은데, 김치찌게 먹으러 갈려고.
교정: 오늘 날씨가 안 좋은데, 김치찌개 먹으러 가려고.

포함 오류:
1. 맞춤법: 않→안, 찌게→찌개
2. 띄어쓰기: 않좋은→안 좋은
3. 문법: 갈려고→가려고
```

조사 예시(1가지만), 띄어쓰기 예시(1가지만)와 대조적.

#### 성공 요인 2: 적절한 복잡도

- 너무 쉽지 않음 → 모델이 과신하지 않음
- 너무 어렵지 않음 → 모델이 위축되지 않음
- 짧지도 길지도 않음 → 길이 편향 없음

#### 성공 요인 3: 운 좋은 버그 (역설)

**grm260797 케이스 발견**:
```
원문: 353자

Baseline 출력: 1460자 (4배 폭발!)
- 메타데이터 포함하여 동일 문장 4번 반복
- "[수정 보완]", "[최종 교정본]", "최종 출력:" 등
- RuleChecklist 후처리가 일부만 제거
- 평가 시 4번 중 일부와 매칭 → TP 증가

Zero-shot 출력: 353자 (깔끔하지만 교정 안 함)
```

**역설**: 더러운 출력 > 깔끔한 출력 (Recall 기준)

총 5개 케이스(4.5%)에서 메타데이터 발생. 의도하지 않은 점수 향상.

---

## 핵심 인사이트 5가지

### 1. Few-shot의 양날의 검

```
예시 개수별 성능:
0개: 보수적 → FN 증가 → 31.91%
1개: 균형 → 최적점 → 34.04% [중요]
4개: 과적합 → 일반화 실패 → 27.66%
```

**교훈**:
- More is not always better
- 예시 개수보다 **예시 품질**
- 단일 예시도 다양성 있으면 충분

### 2. 예시 설계 > 예시 개수

**좋은 예시 (맞춤법)**:
- 다양한 오류 유형 (3가지)
- 자연스러운 문장
- 적절한 길이

**나쁜 예시 (조사/띄어쓰기)**:
- 단일 오류 유형 (1가지)
- 특화된 케이스
- 일반화 실패

**나쁜 Few-shot (Plus3)**:
- 모두 짧은 문장 → 길이 편향
- 띄어쓰기 없음 → 띄어쓰기 소홀
- 단순 패턴만 → 복잡한 케이스 실패

### 3. Train 성능은 거짓말한다

**증거**:

| 프롬프트 | Train | Public | Private | Train-Public 격차 |
|---------|-------|--------|---------|------------------|
| Baseline | 32.24 | 34.04 | 13.45 | **+1.80** (일반화) |
| Zero-shot | 32.24 | 31.91 | 12.61 | -0.33 |
| 조사 | **33.47** | 31.91 | **11.54** | **-1.56** (과적합) |
| Plus3 | **34.69** | 27.66 | 9.77 | **-7.03** (파국적 과적합) |

**패턴**: Train 성능 향상 → Test 성능 하락

**이유**:
- Train 데이터에 과적합
- 분포 차이 (Public/Private 격차 20%p)
- Train 기반 의사결정 불가능

**교훈**: Validation이 아니라 실제 Test 제출만이 진실.

### 4. Baseline의 한계는 규칙이 아닌 이해력

**실험 전 생각**:
"Baseline이 34%밖에 안 되는 건 명확한 규칙을 놓쳐서겠지?"

**실험 후 발견**:
- 명확한 규칙: 100% 처리
- "금새→금세", "치 않→지 않" 등 완벽 교정
- 규칙 추가 시도 → 적용 0개

**진짜 한계**:
- 복잡한 문맥 이해 필요
- 애매한 오류 (주관적 판단)
- 메타데이터 출력 제어 부족

**시사점**: 프롬프트 개선 방향은 규칙 강화가 아니라 **출력 형식 제약** 강화.

### 5. 최적점은 우연히 발견된다

**Baseline 34.04%가 최고인 이유**:
1. 예시의 우수한 설계 (의도적? 우연?)
2. 1개 예시의 균형 (적절한 제약)
3. 메타데이터 버그 (의도하지 않은 이득)

**의도적 설계가 아니었음**.

**교훈**:
- 완벽한 프롬프트를 찾으려는 집착보다
- 다양한 시도와 검증이 중요
- 첫 시도가 최선일 수 있다는 겸손

---

## 실험 전체 요약 테이블

| Phase | 프롬프트 | 예시 | Train | Public | Private | 결과 | 실패 원인 |
|-------|---------|------|-------|--------|---------|------|-----------|
| 1 | **Baseline** | **1개** | 32.24 | **34.04** | **13.45** | [완료] **최고** | - |
| 2 | Zero-shot | 0개 | 32.24 | 31.91 | 12.61 | [실패] 보수적 | FN 증가 |
| 2 | Plus3 | 4개 | 34.69 | 27.66 | 9.77 | [실패] 파국 | 과적합, 길이 폭발 |
| 3 | 조사 | 1개 | 33.47 | 31.91 | 11.54 | [실패] 특화 실패 | 일반화 실패 |
| 3 | 띄어쓰기 | 1개 | 32.65 | - | - | [실패] 폐기 | 길이 폭발 49개 |
| 6 | 규칙 후처리 | - | 32.24 | - | - | [실패] 무효 | 규칙 적용 0개 |

**제출 사용**: 4회 / 20회 (16회 남음)

---

## 기술적 한계 및 극복 시도

### 한계 1: Public/Private 격차 20%p

**현상**: 모든 실험에서 일관되게 발생
```
Baseline: 34.04% → 13.45% (-20.59%p)
Zero-shot: 31.91% → 12.61% (-19.30%p)
Plus3: 27.66% → 9.77% (-17.89%p)
```

**원인 추정**:
- 데이터 분포 차이
- Public: 40%, Private: 60% 비율
- Train 데이터가 Public 치우침 가능성

**시도한 극복**:
- 일반화 전략 (다양성 확보)
- 보수적 접근 (Zero-shot)

**결과**: 모두 실패. 구조적 문제로 판단.

### 한계 2: 메타데이터 출력 제어

**현상**:
- Baseline 5개 케이스(4.5%)에서 메타데이터 포함
- "[수정 보완]", "최종 출력:" 등

**시도한 극복**:
- RuleChecklist postprocessor 구현
- 정규표현식 기반 제거

**결과**: 일부만 제거 (grm260797 등 4x 반복은 일부 잔존)

**역설**: 제거하면 점수 하락 가능성 (TP 감소)

### 한계 3: 토큰 제약

**제약사항**:
- 세션당 2000 토큰
- API 호출 3회/케이스

**영향**:
- Few-shot 예시 개수 제한
- 복잡한 프롬프트 불가
- Multi-turn 어려움

**대응**:
- 예시 개수 최소화 (1개)
- 간결한 프롬프트 설계

---

## 얻은 교훈 및 다음 단계

### 즉시 적용 가능한 교훈

1. **Few-shot은 신중하게**
   - 예시 1-2개가 적정
   - 각 예시는 다양한 패턴 포함
   - 길이/복잡도 균형

2. **Train 성능 맹신 금지**
   - Train 향상 ≠ Test 향상
   - 실제 Test만이 진실
   - Validation 전략 필요

3. **특화보다 일반화**
   - 특정 오류 유형 특화 → 과적합
   - 다양한 오류 유형 균등 → 일반화
   - 커버리지 > 정확도

4. **규칙의 한계 인정**
   - 명확한 규칙은 이미 처리됨
   - 진짜 어려운 건 맥락/뉘앙스
   - 규칙보다 출력 형식 제약

### 시도하지 못한 접근법

**완전히 다른 방향**:
1. System message 강화
2. JSON 구조화 출력 (메타데이터 제거)
3. Multi-turn (오류 탐지 → 교정 분리)
4. 출력 형식 제약 명시 (정규식, 예시)

**리스크**: 모든 개선 시도가 실패했음을 고려

### 대회 후 회고

**성과**:
- Public LB 34.04% 달성
- 체계적 실험 6단계 완료
- 실패로부터 5가지 핵심 인사이트 도출

**아쉬움**:
- Baseline을 넘지 못함
- Public/Private 격차 해소 실패
- 16회 제출 남음 (활용 못 함)

**가치**:
- 프롬프트 엔지니어링의 함정 이해
- Few-shot 설계의 중요성 체득
- 일반화와 과적합의 균형 학습

**최종 결론**:

34.04%는 단순한 숫자가 아니라, 20번의 제출 중 4번을 사용해 얻은 교훈의 집합이다.

완벽한 프롬프트를 찾는 여정이 아니라,
최적점이 어디인지, 왜 그런지 이해하는 과정이었다.

그리고 때로는 **첫 시도가 최선**이라는 겸손함을 배웠다.

---

## 참고 자료

### 주요 문서
- `FINAL_EXPERIMENT_SUMMARY.md`: Phase 1-5 종합 분석
- `FAILURE_ANALYSIS_BASELINE_PLUS_3EXAMPLES.md`: Plus3 실패 상세 분석
- `FINAL_MINIMAL_RULES_EXPERIMENT.md`: Phase 6 규칙 후처리 실험
- `EXPERIMENT_LOG_SUMMARY.md`: 전체 실험 이력

### 제출 파일
- **최종 선택**: `outputs/submissions/test/submission_baseline_test_clean.csv` (34.04 / 13.45)
- Zero-shot: `submission_zero_shot_test.csv` (31.91 / 12.61)
- 조사: `submission_baseline_josa_test.csv` (31.91 / 11.54)
- Plus3: `submission_baseline_plus_3examples_test.csv` (27.66 / 9.77)

### 코드
- Baseline 프롬프트: `code/src/prompts/baseline.py`
- 실패한 변형들: `code/src/prompts/zero_shot.py`, `baseline_plus_3examples.py`, `baseline_josa.py`
- 핵심 실험 스크립트: `code/validate_baseline_minimal_rules.py`, `analyze_fewshot_failure.py`

---

**작성일**: 2025-10-24
**최종 업데이트**: Phase 6 완료 후
**상태**: 최종 확정
