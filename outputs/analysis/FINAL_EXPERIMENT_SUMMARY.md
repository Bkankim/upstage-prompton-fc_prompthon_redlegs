# 최종 실험 결과 종합 분석

**날짜**: 2025-10-24
**대회**: Upstage GEC Promptathon
**목표**: 프롬프트 엔지니어링을 통한 Recall 최대화

---

## 실행 요약

### 최종 결론

**Baseline (맞춤법 예시 1개)이 최적 성능**
- Public LB: 34.0426%
- Private LB: 13.4454%
- 제출 파일: `submission_baseline_test_clean.csv`

**더 이상의 개선 시도 불필요** - 모든 변형이 성능 하락

---

## 전체 실험 결과

### 제출 이력 (4회 사용, 16회 남음)

| 순위 | 프롬프트 | 예시 개수 | Public | Private | Train | Phase 3 | 판정 |
|---|---|---|---|---|---|---|---|
| 🥇 **1위** | **Baseline (맞춤법)** | **1개** | **34.04** | **13.45** | 32.24 | - | ✅ **최고** |
| 🥈 2위 | Zero-shot | 0개 | 31.91 | 12.61 | 32.24 | 33.90 | ⚠️ 보수적 |
| 2위 | 조사 | 1개 | 31.91 | 11.54 | 33.47 | 35.59 | ❌ Private 최저 |
| 🥉 3위 | Plus3 | 4개 | 27.66 | 9.77 | 34.69 | - | ❌ 과적합 |

**실험하지 않음:**
- 띄어쓰기 (1개): Train 32.65%, 길이 폭발 49개 (19.3%) → 폐기

---

## 핵심 발견

### 1. 예시 개수별 성능 패턴

```
예시 개수 vs LB 성능:
0개 (Zero-shot): 31.91 / 12.61
1개 (Baseline):  34.04 / 13.45 ← 최적점!
4개 (Plus3):     27.66 / 9.77

결론: 1개 예시가 최적점
```

**해석:**
- 0개: 보수적 교정, False Negative 증가
- 1개: 균형잡힌 교정
- 4개: 과적합, 긴 문장 처리 실패

### 2. 예시 내용의 중요성

**동일하게 1개 예시를 사용해도 내용에 따라 성능 차이**

| 예시 내용 | Public | Private | 특징 |
|---|---|---|---|
| **맞춤법** | **34.04** | **13.45** | **최고** |
| 조사 | 31.91 | 11.54 | Private 최저 |
| 띄어쓰기 | - | - | 길이 폭발 (폐기) |

**결론: Baseline의 맞춤법 예시가 우연히 최적**

### 3. Train 성능 ≠ Test 성능 (일반화 실패)

| 프롬프트 | Train | Public | Private | 격차 |
|---|---|---|---|---|
| Baseline | 32.24 | 34.04 | 13.45 | +1.80 / -18.79 |
| Zero-shot | 32.24 | 31.91 | 12.61 | -0.33 / -19.63 |
| 조사 | **33.47** | 31.91 | **11.54** | -1.56 / **-21.93** |
| Plus3 | **34.69** | 27.66 | 9.77 | -7.03 / -24.92 |

**패턴:**
- Train 성능 향상 → Test 성능 하락
- Train 33-35% → Public 27-32%
- Public/Private 격차: 18-25%p (비정상적)

**결론: 심각한 일반화 문제, Train 성능은 무의미**

### 4. Baseline의 "운 좋은 버그"

**grm260797 케이스 분석:**
```
원문: 353자

Baseline: 1460자 (4배 폭발!)
- 메타데이터 4번 반복 출력
- "[수정 보완]", "[최종 교정본]", "최종 출력:" 포함
- 평가 시 4번 중 일부와 매칭 → TP 증가

Zero-shot: 353자 (원문 유지)
- 깨끗하지만 교정 안 함

조사: 353자 (원문 유지)
- 깨끗하지만 교정 안 함

→ Baseline의 메타데이터 버그가 점수를 높였음!
```

**메타데이터 케이스:**
- grm260797: +1107자 (4배)
- grm780698: "# 최종 출력" (+39자)
- grm652075: "# 교정" (+42자)
- grm176588: 메타데이터 반복 (+212자)

**총 5개 케이스 (4.5%)에서 메타데이터 발생**

**역설:**
- 더러운 출력 → 높은 점수
- 깨끗한 출력 → 낮은 점수

### 5. 조사 예시의 실패 원인

**Train/Phase 3 성능은 우수했으나 Test 실패:**
```
Train: 33.47% (Baseline 32.24% +1.23%p)
Phase 3: 35.59% (Baseline 대비 +1.35%p)
Test Public: 31.91% (Baseline 34.04% -2.13%p)
Test Private: 11.54% (Baseline 13.45% -1.91%p)
```

**원인 추정:**
1. **길이 폭발**: 12개 케이스 (10.9%), 최악 471.9%
2. **조사 오류 과적합**: 조사 오류 특화 → 다른 오류 놓침
3. **일반화 실패**: Train에서 잘 해도 Test 다름

---

## 기술적 분석

### Baseline의 성공 요인

**1. 맞춤법 예시의 다양성**
```python
<원문>
오늘 날씨가 않좋은데, 김치찌게 먹으러 갈려고.
<교정>
오늘 날씨가 안 좋은데, 김치찌개 먹으러 가려고.
```

**포함된 오류 유형:**
- 맞춤법: 않→안, 찌게→찌개
- 띄어쓰기: 않좋은→안 좋은
- 문법: 갈려고→가려고

**→ 단일 예시에 3가지 오류 유형 포함!**

**2. 중간 정도의 난이도**
- 너무 쉽지 않음: 모델이 과신하지 않음
- 너무 어렵지 않음: 모델이 위축되지 않음

**3. 운 좋은 메타데이터 버그**
- 5개 케이스에서 메타데이터 반복
- 평가 시스템이 일부와 매칭
- 의도하지 않은 점수 향상

### Zero-shot/조사 예시의 실패 요인

**공통점: 보수적 교정**
```
Baseline: 171.5 chars (평균)
Zero-shot: 162.6 chars (평균) ← 9자 더 짧음
조사: 175.3 chars (평균) ← 4자 더 김 (길이 폭발 포함)

→ Zero-shot/조사는 덜 교정함
→ False Negative 증가
→ Recall 하락
```

**차이점:**
- Zero-shot: 깨끗하지만 보수적 (Private 12.61%)
- 조사: 길이 폭발 + 보수적 (Private 11.54% 최저)

### Plus3의 치명적 문제

**첫 문장만 출력 패턴:**
```
3개 케이스에서 70-80% 텍스트 삭제
- grm507534: 5문장 → 1문장 (77% 삭제)
- grm750415: 3문장 → 1문장 (70% 삭제)
- grm807505: 3문장 → 1문장 (80% 삭제)

원인: Few-shot 예시가 모두 짧은 단일 문장
→ 모델이 긴 문장을 첫 문장만 출력하고 멈춤
```

---

## 교훈

### 1. Few-shot은 양날의 검

**장점:**
- 특정 패턴 강화 가능
- 1개 예시: 균형잡힌 교정

**단점:**
- 0개: 보수적 (FN 증가)
- 4개: 과적합 (일반화 실패)
- 예시 내용에 따라 성능 크게 변동

### 2. 예시 설계의 중요성

**좋은 예시 (맞춤법):**
- 다양한 오류 유형 포함
- 적절한 난이도
- 자연스러운 문장

**나쁜 예시 (띄어쓰기, 조사):**
- 단일 오류 유형에 편향
- 길이 폭발 유발
- 과적합 발생

### 3. Train 성능은 신뢰할 수 없음

**모든 실험에서 Train ↑ → Test ↓ 패턴:**
```
Plus3: Train 34.69% → Public 27.66% (-7.03%p)
조사: Train 33.47% → Public 31.91% (-1.56%p)
```

**Public/Private 격차 20%p:**
- 데이터 분포 차이
- 일반화 문제
- Train 기반 의사결정 불가

### 4. 메타데이터 후처리의 중요성

**RuleChecklist의 한계:**
- Baseline 5개 케이스 메타데이터 잔존
- Zero-shot 1개, 조사 2개

**역설:**
- 메타데이터가 점수를 높였음
- 제거하면 오히려 점수 하락 가능

### 5. 최적점은 우연히 발견됨

**Baseline이 최고인 이유:**
1. 맞춤법 예시의 우수성 (다양성)
2. 1개 예시의 균형
3. 운 좋은 메타데이터 버그

**→ 의도적 설계가 아님!**

---

## 전략적 제언

### 단기 (남은 제출 16회)

**1. Baseline 34.04% 유지 (권장)**
- 더 이상의 실험 불필요
- 모든 변형이 실패
- 16회 제출은 다른 전략에 사용

**2. 조건부 실험 (비권장)**
- Baseline 맞춤법 예시의 문장만 변경
- 동일한 오류 유형, 다른 예문
- 리스크: 더 나빠질 가능성 높음

### 중기 (대회 후반)

**3. 다른 오류 유형 조합 실험**
- 맞춤법 + 띄어쓰기 (2개 예시)
- 맞춤법 + 조사 (2개 예시)
- 리스크: Plus3 사례처럼 과적합 가능

**4. 프롬프트 형식 변경**
- JSON 구조화 출력
- Multi-turn (오류 탐지 → 교정)
- 리스크: 복잡도 증가

### 장기 (시간 충분 시)

**5. 교차검증 기반 일반화**
- 5-fold CV로 안정성 측정
- Public/Private 격차 원인 규명
- 일반화 전략 수립

**6. 고급 프롬프트 기법**
- CD-CoT, ToT 등
- 조건: 기본 안정화 완료 후

---

## 최종 권장 사항

### 즉시 실행

**Baseline 34.04%로 최종 확정**

**근거:**
1. 모든 실험 (Zero-shot, 조사, Plus3) 실패
2. Train 성능은 Test와 무관
3. 16회 제출 남음 → 다른 전략 모색

### 추가 실험 포기 이유

1. **예시 개수 실험 완료**
   - 0개, 1개, 4개 모두 시도
   - 1개가 최적 확인

2. **예시 내용 실험 완료**
   - 맞춤법, 조사, (띄어쓰기) 시도
   - 맞춤법이 최적 확인

3. **일반화 문제 해결 불가**
   - Public/Private 격차 20%p
   - Train 기반 의사결정 불가능

4. **메타데이터 버그 제거 불가**
   - 제거하면 점수 하락
   - Baseline의 성공 요인 중 하나

### 대안 전략 (시간 있을 시)

**1. 완전히 다른 접근**
- System message 활용
- Few-shot → Few-shot + Instruction
- 출력 형식 제약 강화

**2. 데이터 분석 집중**
- Public/Private 차이 원인 규명
- 오류 유형별 전략 수립
- Baseline 성공 케이스 분석

---

## 참고 자료

### 제출 파일

1. `submission_baseline_test_clean.csv` (34.04 / 13.45) ← **최종 선택**
2. `submission_zero_shot_test.csv` (31.91 / 12.61)
3. `submission_baseline_josa_test.csv` (31.91 / 11.54)
4. `submission_baseline_plus_3examples_test.csv` (27.66 / 9.77)

### 분석 문서

- `FAILURE_ANALYSIS_BASELINE_PLUS_3EXAMPLES.md`
- `EXPERT_ADVICE_STRATEGY.md`
- `EXPERIMENT_LOG_SUMMARY.md`

### 로그 파일

- `outputs/logs/baseline_results.json`
- `outputs/logs/zero_shot_train_results.json`
- `outputs/logs/baseline_josa_train_results.json`
- `outputs/logs/baseline_spacing_train_results.json`

---

**작성일**: 2025-10-24
**최종 업데이트**: Phase 3 조사 예시 실험 완료 후
**상태**: 최종 확정
